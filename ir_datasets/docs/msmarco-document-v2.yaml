_:
  pretty_name: 'MSMARCO (document, version 2)'
  desc: '
  <p>
Version 2 of the MS MARCO document ranking dataset. The corpus contains 12M documents (roughly 3x
as many as version 1).
</p>
<ul>
  <li>Version 1 of dataset: <a class="ds-ref">msmarco-document</a></li>
  <li>Documents: Text extracted from web pages</li>
  <li>Queries: Natural language questions (from query log)</li>
  <li><a href="https://arxiv.org/abs/1611.09268">Dataset Paper</a></li>
</ul>'
  bibtex_ids: ['Bajaj2016Msmarco']


dev1:
  desc: '
<p>
Official dev1 set with 4,552 queries.
</p>
'
  bibtex_ids: ['Bajaj2016Msmarco']

dev2:
  desc: '
<p>
Official dev2 set with 5,000 queries.
</p>
'
  bibtex_ids: ['Bajaj2016Msmarco']

train:
  desc: '
<p>
Official train set with 322,196 queries.
</p>
'
  bibtex_ids: ['Bajaj2016Msmarco']


trec-dl-2019:
  desc: '
<p>
Queries from the TREC Deep Learning (DL) 2019 shared task, which were sampled from
<a class="ds-ref">msmarco-document/eval</a>. A subset of these queries were judged by NIST assessors,
(filtered list available in <a class="ds-ref">msmarco-document-v2/trec-dl-2019/judged</a>).
</p>
<ul>
<li><a href="https://arxiv.org/pdf/2003.07820.pdf">Shared Task Paper</a></li>
</ul>
'
  bibtex_ids: ['Craswell2019TrecDl', 'Bajaj2016Msmarco']


trec-dl-2019/judged:
  desc: '
<p>
Subset of <a class="ds-ref">msmarco-document-v2/trec-dl-2019</a>, only including queries with qrels.
</p>
'
  bibtex_ids: ['Craswell2019TrecDl', 'Bajaj2016Msmarco']


trec-dl-2020:
  desc: '
<p>
Queries from the TREC Deep Learning (DL) 2020 shared task, which were sampled from
<a class="ds-ref">msmarco-document/eval</a>. A subset of these queries were judged by NIST assessors,
(filtered list available in <a class="ds-ref">msmarco-document-v2/trec-dl-2020/judged</a>).
</p>
<ul>
<li><a href="https://arxiv.org/pdf/2102.07662.pdf">Shared Task Paper</a></li>
</ul>
'
  bibtex_ids: ['Craswell2020TrecDl', 'Bajaj2016Msmarco']

trec-dl-2020/judged:
  desc: '
<p>
Subset of <a class="ds-ref">msmarco-document-v2/trec-dl-2020</a>, only including queries with qrels.
</p>
'
  bibtex_ids: ['Craswell2020TrecDl', 'Bajaj2016Msmarco']

trec-dl-2021:
  desc: '
<p>
Official topics for the TREC Deep Learning (DL) 2021 shared task.
</p>
<p>
Note that at this time, qrels are only available to those with TREC active participant login credentials.
</p>
'
  official_measures: ['AP@100', 'nDCG@10', 'P@10', 'RR(rel=2)']

trec-dl-2021/judged:
  desc: '
<p>
<a class="ds-ref">msmarco-document-v2/trec-dl-2021</a>, but filtered down to the 57 queries
with qrels.
</p>
<p>
Note that at this time, this is only available to those with TREC active participant login credentials.
</p>
'
  official_measures: ['AP@100', 'nDCG@10', 'P@10', 'RR(rel=2)']

trec-dl-2022:
  desc: '
<p>
Official topics for the TREC Deep Learning (DL) 2022 shared task.
</p>
<p>
Note that these qrels are <i>inferred</i> from the passage ranking task; a document''s relevance
label is the maximum of the labels of its passages.
</p>
'

trec-dl-2022/judged:
  desc: '
<p>
<a class="ds-ref">msmarco-document-v2/trec-dl-2022</a>, but filtered down to only the queries
with qrels.
</p>
'

trec-dl-2023:
  desc: '
<p>
Official topics for the TREC Deep Learning (DL) 2023 shared task.
</p>
'

anchor-text:
  pretty_name: "Anchor Text for version 2 of MS Marco"
  desc: '
<p>
For version 2 of MS MARCO, the anchor text collection enriches 4,821,244 documents with anchor text extracted from six Common Crawl snapshots. To keep the collection size reasonable, we sampled 1,000 anchor texts for documents with more than 1,000 anchor texts (this sampling yields that all anchor text is included for 97% of the documents). The <code>text</code> field contains the anchor texts concatenated and the <code>anchors</code> field contains the anchor texts as list. The raw dataset with additional information (roughly 100GB) is <a href="https://github.com/webis-de/ecir22-anchor-text">available online</a>.
</p>
'
  bibtex_ids: ['Froebe2022Anchors']
